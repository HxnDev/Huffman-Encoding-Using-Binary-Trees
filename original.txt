Youve probably heard about David Huffman and his popular compression algorithm. The idea behind Huffman coding is based upon the frequency of a symbol in a sequence. The symbol that is the most frequent in that sequence gets a new code that is very small, the least frequent symbol will get a code that is very long, so that when well translate the input we want to encode the most frequent symbols will take less space than they used to and the least frequent symbols will take more space but because they’re less frequent it won’t matter that much.For this algorithm you need to have a basic understanding of binary tree data structure and the ordered queue (sorted on the bases of frequency) data structure. In the source code well use the ordered queue. Lets say we have the string beep boop beer! which in his actual form occupies 7 bits (suppose ascii 0 to 127 ). That means that in total it occupies 15+7 = 105 bits of memory. Through encoding the string will occupy 40 bits approximately. To better understand this example well going to apply it on an example. The string beep boop beer! is a very good example to illustrate this. In order to obtain the code for each element depending on its frequency well need to build a binary tree such that each leaf of the tree will contain a symbol (a character from the string). The tree will be built from the leafs to the root, meaning that the elements of least frequency will be farther from the root than the elements that are more frequent. To build the tree this way well use a ordered queue, that the element with the least frequency is the most important. Meaning that the elements that are the least frequent will be the first ones we get from the queue. We need to do this so we can build the tree from the leaves to the root.